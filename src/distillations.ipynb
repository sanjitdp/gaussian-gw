{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a53905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import scipy\n",
    "import pickle\n",
    "from scipy.linalg import eigh, sqrtm, qr\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def calculate_bert_params(layers, hidden_size):\n",
    "    vocab_size = 30522\n",
    "    max_position_embeddings = 512\n",
    "    type_vocab_size = 2\n",
    "    intermediate_size = 4 * hidden_size\n",
    "\n",
    "    embedding_params = hidden_size * (\n",
    "        vocab_size + max_position_embeddings + type_vocab_size\n",
    "    )\n",
    "\n",
    "    layer_params = (\n",
    "        (4 * hidden_size**2) + (2 * hidden_size * intermediate_size) + (2 * hidden_size)\n",
    "    )\n",
    "\n",
    "    total_params = embedding_params + (layers * layer_params)\n",
    "    return total_params / 1_000_000\n",
    "\n",
    "\n",
    "BERT_MODELS = {\n",
    "    \"bert-2-128\": (\"google/bert_uncased_L-2_H-128_A-2\", calculate_bert_params(2, 128)),\n",
    "    \"bert-2-256\": (\"google/bert_uncased_L-2_H-256_A-4\", calculate_bert_params(2, 256)),\n",
    "    \"bert-2-512\": (\"google/bert_uncased_L-2_H-512_A-8\", calculate_bert_params(2, 512)),\n",
    "    \"bert-2-768\": (\"google/bert_uncased_L-2_H-768_A-12\", calculate_bert_params(2, 768)),\n",
    "    \"bert-4-128\": (\"google/bert_uncased_L-4_H-128_A-2\", calculate_bert_params(4, 128)),\n",
    "    \"bert-4-256\": (\n",
    "        \"google/bert_uncased_L-4_H-256_A-4\",\n",
    "        calculate_bert_params(4, 256),\n",
    "    ),\n",
    "    \"bert-4-512\": (\n",
    "        \"google/bert_uncased_L-4_H-512_A-8\",\n",
    "        calculate_bert_params(4, 512),\n",
    "    ),\n",
    "    \"bert-4-768\": (\"google/bert_uncased_L-4_H-768_A-12\", calculate_bert_params(4, 768)),\n",
    "    \"bert-6-128\": (\"google/bert_uncased_L-6_H-128_A-2\", calculate_bert_params(6, 128)),\n",
    "    \"bert-6-256\": (\"google/bert_uncased_L-6_H-256_A-4\", calculate_bert_params(6, 256)),\n",
    "    \"bert-6-512\": (\"google/bert_uncased_L-6_H-512_A-8\", calculate_bert_params(6, 512)),\n",
    "    \"bert-6-768\": (\"google/bert_uncased_L-6_H-768_A-12\", calculate_bert_params(6, 768)),\n",
    "    \"bert-8-128\": (\"google/bert_uncased_L-8_H-128_A-2\", calculate_bert_params(8, 128)),\n",
    "    \"bert-8-256\": (\"google/bert_uncased_L-8_H-256_A-4\", calculate_bert_params(8, 256)),\n",
    "    \"bert-8-512\": (\n",
    "        \"google/bert_uncased_L-8_H-512_A-8\",\n",
    "        calculate_bert_params(8, 512),\n",
    "    ),\n",
    "    \"bert-8-768\": (\"google/bert_uncased_L-8_H-768_A-12\", calculate_bert_params(8, 768)),\n",
    "    \"bert-10-128\": (\n",
    "        \"google/bert_uncased_L-10_H-128_A-2\",\n",
    "        calculate_bert_params(10, 128),\n",
    "    ),\n",
    "    \"bert-10-256\": (\n",
    "        \"google/bert_uncased_L-10_H-256_A-4\",\n",
    "        calculate_bert_params(10, 256),\n",
    "    ),\n",
    "    \"bert-10-512\": (\n",
    "        \"google/bert_uncased_L-10_H-512_A-8\",\n",
    "        calculate_bert_params(10, 512),\n",
    "    ),\n",
    "    \"bert-10-768\": (\n",
    "        \"google/bert_uncased_L-10_H-768_A-12\",\n",
    "        calculate_bert_params(10, 768),\n",
    "    ),\n",
    "    \"bert-12-128\": (\n",
    "        \"google/bert_uncased_L-12_H-128_A-2\",\n",
    "        calculate_bert_params(12, 128),\n",
    "    ),\n",
    "    \"bert-12-256\": (\n",
    "        \"google/bert_uncased_L-12_H-256_A-4\",\n",
    "        calculate_bert_params(12, 256),\n",
    "    ),\n",
    "    \"bert-12-512\": (\n",
    "        \"google/bert_uncased_L-12_H-512_A-8\",\n",
    "        calculate_bert_params(12, 512),\n",
    "    ),\n",
    "    \"bert-12-768\": (\"bert-base-uncased\", 110.0),\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    \"amazon_polarity\": (\"amazon_polarity\", \"test[:1000]\"),\n",
    "    \"yelp_review\": (\"yelp_review_full\", \"test[:1000]\"),\n",
    "    \"imdb\": (\"imdb\", \"test[:1000]\"),\n",
    "    \"ag_news\": (\"ag_news\", \"test[:1000]\"),\n",
    "}\n",
    "\n",
    "\n",
    "def stiefel_objective(X, Lambda1, Lambda2, v1, v2):\n",
    "    term1 = np.trace(Lambda1 @ X @ Lambda2 @ X.T)\n",
    "    term2 = 2 * v1.T @ X @ v2\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def stiefel_gradient(X, Lambda1, Lambda2, v1, v2):\n",
    "    grad = 2 * Lambda1 @ X @ Lambda2 + 2 * np.outer(v1, v2)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def stiefel_riemannian_gradient(X, euclidean_grad):\n",
    "    XTgrad = X.T @ euclidean_grad\n",
    "    sym_XTgrad = 0.5 * (XTgrad + XTgrad.T)\n",
    "    riem_grad = euclidean_grad - X @ sym_XTgrad\n",
    "    return riem_grad\n",
    "\n",
    "\n",
    "def stiefel_retraction(X, direction, step_size):\n",
    "    Y = X + step_size * direction\n",
    "\n",
    "    if Y.shape[0] == Y.shape[1]:\n",
    "        Q, R = qr(Y)\n",
    "    else:\n",
    "        Q, R = qr(Y, mode=\"economic\")\n",
    "\n",
    "    signs = np.sign(np.diag(R))\n",
    "    signs[signs == 0] = 1\n",
    "    Q = Q @ np.diag(signs)\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def optimize_stiefel(\n",
    "    Lambda1,\n",
    "    Lambda2,\n",
    "    v1,\n",
    "    v2,\n",
    "    d1,\n",
    "    d2,\n",
    "    max_iter=50,\n",
    "    tol=1e-2,\n",
    "    step_size=0.01,\n",
    "    verbose=False,\n",
    "):\n",
    "    np.random.seed(42)\n",
    "    X = np.eye(d1, d2)\n",
    "\n",
    "    objective_history = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        obj = stiefel_objective(X, Lambda1, Lambda2, v1, v2)\n",
    "        objective_history.append(obj)\n",
    "\n",
    "        euclidean_grad = stiefel_gradient(X, Lambda1, Lambda2, v1, v2)\n",
    "        riem_grad = stiefel_riemannian_gradient(X, euclidean_grad)\n",
    "\n",
    "        grad_norm = np.linalg.norm(riem_grad)\n",
    "        if grad_norm < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "\n",
    "        X_new = stiefel_retraction(X, riem_grad, step_size)\n",
    "        obj_new = stiefel_objective(X_new, Lambda1, Lambda2, v1, v2)\n",
    "\n",
    "        backtrack_count = 0\n",
    "        while obj_new < obj and step_size > 1e-12 and backtrack_count < 10:\n",
    "            step_size *= 0.5\n",
    "            X_new = stiefel_retraction(X, riem_grad, step_size)\n",
    "            obj_new = stiefel_objective(X_new, Lambda1, Lambda2, v1, v2)\n",
    "            backtrack_count += 1\n",
    "\n",
    "        if obj_new >= obj:\n",
    "            X = X_new\n",
    "            step_size = min(step_size * 1.05, 0.1)\n",
    "        else:\n",
    "            step_size *= 0.5\n",
    "\n",
    "    return X, objective_history\n",
    "\n",
    "\n",
    "def compute_igw_distance(m1, m2, Sigma1, Sigma2, verbose=False):\n",
    "    d1_orig, d2_orig = len(m1), len(m2)\n",
    "\n",
    "    swapped = False\n",
    "    if d1_orig < d2_orig:\n",
    "        m1, m2 = m2, m1\n",
    "        Sigma1, Sigma2 = Sigma2, Sigma1\n",
    "        swapped = True\n",
    "\n",
    "    d1, d2 = len(m1), len(m2)\n",
    "\n",
    "    eig1 = np.linalg.eigvals(Sigma1)\n",
    "    eig2 = np.linalg.eigvals(Sigma2)\n",
    "\n",
    "    if not np.all(eig1 > 1e-12):\n",
    "        Sigma1 += np.eye(d1) * 1e-6\n",
    "    if not np.all(eig2 > 1e-12):\n",
    "        Sigma2 += np.eye(d2) * 1e-6\n",
    "\n",
    "    Lambda1_vals, Q1 = eigh(Sigma1)\n",
    "    Lambda2_vals, Q2 = eigh(Sigma2)\n",
    "\n",
    "    Lambda1_vals = np.maximum(Lambda1_vals, 1e-12)\n",
    "    Lambda2_vals = np.maximum(Lambda2_vals, 1e-12)\n",
    "\n",
    "    def safe_diag(vals):\n",
    "        vals = np.atleast_1d(vals)\n",
    "        if len(vals) == 1:\n",
    "            return np.array([[vals[0]]])\n",
    "        else:\n",
    "            return np.diag(vals)\n",
    "\n",
    "    Lambda1 = safe_diag(Lambda1_vals)\n",
    "    Lambda2 = safe_diag(Lambda2_vals)\n",
    "\n",
    "    m1_tilde = Q1.T @ m1\n",
    "    m2_tilde = Q2.T @ m2\n",
    "\n",
    "    v1 = np.sqrt(Lambda1_vals) * m1_tilde\n",
    "    v2 = np.sqrt(Lambda2_vals) * m2_tilde\n",
    "\n",
    "    X_opt, obj_history = optimize_stiefel(\n",
    "        Lambda1, Lambda2, v1, v2, d1, d2, verbose=verbose\n",
    "    )\n",
    "\n",
    "    gamma_star = stiefel_objective(X_opt, Lambda1, Lambda2, v1, v2)\n",
    "\n",
    "    term1 = np.sum(Lambda1_vals**2)\n",
    "    term2 = np.sum(Lambda2_vals**2)\n",
    "    term3 = 2 * np.sum(Lambda1_vals * m1_tilde**2)\n",
    "    term4 = 2 * np.sum(Lambda2_vals * m2_tilde**2)\n",
    "    term5 = (\n",
    "        np.linalg.norm(m1_tilde) ** 2 - np.linalg.norm(m2_tilde) ** 2\n",
    "    ) ** 2\n",
    "\n",
    "    igw_distance_squared = term1 + term2 + term3 + term4 + term5 - 2 * gamma_star\n",
    "    igw_distance = np.sqrt(max(0, igw_distance_squared))\n",
    "\n",
    "    return igw_distance\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\", leave=False):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def project_to_common_dimension(embeddings, target_dim=128):\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    if embeddings.shape[1] <= target_dim:\n",
    "        if embeddings.shape[1] < target_dim:\n",
    "            padding = np.zeros((embeddings.shape[0], target_dim - embeddings.shape[1]))\n",
    "            embeddings = np.hstack([embeddings, padding])\n",
    "        return embeddings\n",
    "    else:\n",
    "        pca = PCA(n_components=target_dim)\n",
    "        return pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def fit_gaussian(embeddings, target_dim=128):\n",
    "    embeddings_proj = project_to_common_dimension(embeddings, target_dim)\n",
    "\n",
    "    mean = np.mean(embeddings_proj, axis=0)\n",
    "    cov = np.cov(embeddings_proj.T) + np.eye(embeddings_proj.shape[1]) * 1e-6\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def compute_igw_bounds(mean1, cov1, mean2, cov2):\n",
    "    d1_orig, d2_orig = len(mean1), len(mean2)\n",
    "\n",
    "    if d1_orig < d2_orig:\n",
    "        mean1, mean2 = mean2, mean1\n",
    "        cov1, cov2 = cov2, cov1\n",
    "\n",
    "    eig1 = np.linalg.eigvals(cov1)\n",
    "    eig2 = np.linalg.eigvals(cov2)\n",
    "\n",
    "    if not np.all(eig1 > 1e-12):\n",
    "        cov1 += np.eye(len(mean1)) * 1e-6\n",
    "    if not np.all(eig2 > 1e-12):\n",
    "        cov2 += np.eye(len(mean2)) * 1e-6\n",
    "\n",
    "    Lambda1_vals, Q1 = eigh(cov1)\n",
    "    Lambda2_vals, Q2 = eigh(cov2)\n",
    "\n",
    "    Lambda1_vals = np.maximum(Lambda1_vals, 1e-12)\n",
    "    Lambda2_vals = np.maximum(Lambda2_vals, 1e-12)\n",
    "\n",
    "    m1_tilde = Q1.T @ mean1\n",
    "    m2_tilde = Q2.T @ mean2\n",
    "\n",
    "    tr_Lambda1_sq = np.sum(Lambda1_vals**2)\n",
    "    tr_Lambda2_sq = np.sum(Lambda2_vals**2)\n",
    "\n",
    "    m1_tilde_Lambda1_m1_tilde = np.sum(Lambda1_vals * m1_tilde**2)\n",
    "    m2_tilde_Lambda2_m2_tilde = np.sum(Lambda2_vals * m2_tilde**2)\n",
    "\n",
    "    norm_m1_tilde_sq = np.linalg.norm(m1_tilde) ** 2\n",
    "    norm_m2_tilde_sq = np.linalg.norm(m2_tilde) ** 2\n",
    "    norm_diff_tilde_sq = (norm_m1_tilde_sq - norm_m2_tilde_sq) ** 2\n",
    "\n",
    "    Lambda1_sorted = np.sort(Lambda1_vals)[::-1]\n",
    "    Lambda2_sorted = np.sort(Lambda2_vals)[::-1]\n",
    "\n",
    "    max_len = max(len(Lambda1_sorted), len(Lambda2_sorted))\n",
    "    if len(Lambda1_sorted) < max_len:\n",
    "        Lambda1_sorted = np.pad(Lambda1_sorted, (0, max_len - len(Lambda1_sorted)))\n",
    "    if len(Lambda2_sorted) < max_len:\n",
    "        Lambda2_sorted = np.pad(Lambda2_sorted, (0, max_len - len(Lambda2_sorted)))\n",
    "\n",
    "    sum_eigenval_products = np.sum(Lambda1_sorted * Lambda2_sorted)\n",
    "\n",
    "    xi = (\n",
    "        tr_Lambda1_sq\n",
    "        + tr_Lambda2_sq\n",
    "        + 2 * m1_tilde_Lambda1_m1_tilde\n",
    "        + 2 * m2_tilde_Lambda2_m2_tilde\n",
    "        + norm_diff_tilde_sq\n",
    "        - 2 * sum_eigenval_products\n",
    "    )\n",
    "\n",
    "    Lambda1_sqrt_m1_tilde = np.sqrt(Lambda1_vals) * m1_tilde\n",
    "    Lambda2_sqrt_m2_tilde = np.sqrt(Lambda2_vals) * m2_tilde\n",
    "\n",
    "    norm_product = np.linalg.norm(Lambda1_sqrt_m1_tilde) * np.linalg.norm(\n",
    "        Lambda2_sqrt_m2_tilde\n",
    "    )\n",
    "\n",
    "    if len(Lambda1_sqrt_m1_tilde) == len(Lambda2_sqrt_m2_tilde):\n",
    "        inner_product = Lambda1_sqrt_m1_tilde.T @ Lambda2_sqrt_m2_tilde\n",
    "    else:\n",
    "        max_dim = max(len(Lambda1_sqrt_m1_tilde), len(Lambda2_sqrt_m2_tilde))\n",
    "        v1_padded = np.pad(\n",
    "            Lambda1_sqrt_m1_tilde, (0, max(0, max_dim - len(Lambda1_sqrt_m1_tilde)))\n",
    "        )\n",
    "        v2_padded = np.pad(\n",
    "            Lambda2_sqrt_m2_tilde, (0, max(0, max_dim - len(Lambda2_sqrt_m2_tilde)))\n",
    "        )\n",
    "        inner_product = v1_padded.T @ v2_padded\n",
    "\n",
    "    gamma_upper_bound = sum_eigenval_products + 2 * norm_product\n",
    "    gamma_lower_bound = sum_eigenval_products - 2 * norm_product\n",
    "\n",
    "    lower_bound_sq = xi - 2 * gamma_upper_bound\n",
    "    upper_bound_sq = xi - 2 * gamma_lower_bound\n",
    "\n",
    "    lower_bound = np.sqrt(max(0, lower_bound_sq))\n",
    "    upper_bound = np.sqrt(max(0, upper_bound_sq))\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_name, dataset_config):\n",
    "    try:\n",
    "        if dataset_config[1].startswith(\"test[\"):\n",
    "            split_name = dataset_config[1]\n",
    "            dataset = load_dataset(dataset_config[0], split=split_name)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_config[0], split=dataset_config[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if \"text\" in dataset.features:\n",
    "        texts = dataset[\"text\"]\n",
    "    elif \"label\" in dataset.features and len(dataset.features) == 2:\n",
    "        text_column = [col for col in dataset.features if col != \"label\"][0]\n",
    "        texts = dataset[text_column]\n",
    "    else:\n",
    "        text_columns = [\n",
    "            col for col, feat in dataset.features.items() if feat.dtype == \"string\"\n",
    "        ]\n",
    "        if not text_columns:\n",
    "            print(f\"No text column found in {dataset_name}\")\n",
    "            return None\n",
    "        texts = dataset[text_columns[0]]\n",
    "\n",
    "    if len(texts) > 2000:\n",
    "        texts = texts[:2000]\n",
    "\n",
    "    model_sizes = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    estimates = []\n",
    "    model_data = {}\n",
    "\n",
    "    all_embeddings = {}\n",
    "    max_dim = 0\n",
    "\n",
    "    for model_name, (model_path, param_count) in tqdm(\n",
    "        BERT_MODELS.items(), desc=\"Embedding\"\n",
    "    ):\n",
    "        try:\n",
    "            model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "\n",
    "            embeddings = get_embeddings(texts, model, tokenizer)\n",
    "            all_embeddings[model_name] = (embeddings, param_count)\n",
    "            max_dim = max(max_dim, embeddings.shape[1])\n",
    "\n",
    "            del model, tokenizer\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    gaussians = {}\n",
    "    for model_name, (embeddings, param_count) in all_embeddings.items():\n",
    "        mean, cov = fit_gaussian(embeddings, max_dim)\n",
    "        gaussians[model_name] = (mean, cov, param_count)\n",
    "\n",
    "    if \"bert-12-768\" not in gaussians:\n",
    "        ref_model = max(gaussians.keys(), key=lambda x: gaussians[x][2])\n",
    "    else:\n",
    "        ref_model = \"bert-12-768\"\n",
    "\n",
    "    base_mean, base_cov, _ = gaussians[ref_model]\n",
    "    print(f\"Using {ref_model} as reference model\")\n",
    "\n",
    "    for model_name, (mean, cov, param_count) in gaussians.items():\n",
    "        if model_name == ref_model:\n",
    "            continue\n",
    "\n",
    "        lower, upper = compute_igw_bounds(mean, cov, base_mean, base_cov)\n",
    "        try:\n",
    "            est = compute_igw_distance(\n",
    "                mean, base_mean, cov, base_cov, verbose=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            est = np.nan\n",
    "\n",
    "        model_sizes.append(param_count)\n",
    "        lower_bounds.append(lower)\n",
    "        upper_bounds.append(upper)\n",
    "        estimates.append(est)\n",
    "\n",
    "        model_data[model_name] = {\n",
    "            \"mean\": mean,\n",
    "            \"covariance\": cov,\n",
    "            \"param_count\": param_count,\n",
    "            \"lower_bound\": lower,\n",
    "            \"upper_bound\": upper,\n",
    "            \"estimate\": est,\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"{model_name}: IGW bounds [{lower:.4f}, {upper:.4f}], Corrected IGW: {est:.4f}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"model_sizes\": model_sizes,\n",
    "        \"lower_bounds\": lower_bounds,\n",
    "        \"upper_bounds\": upper_bounds,\n",
    "        \"estimates\": estimates,\n",
    "        \"model_data\": model_data,\n",
    "        \"reference_model\": ref_model,\n",
    "        \"reference_mean\": base_mean,\n",
    "        \"reference_covariance\": base_cov,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results, filename=\"cache/bert_igw_results.pkl\"):\n",
    "    save_data = {\n",
    "        \"datasets\": {},\n",
    "        \"bert_models\": BERT_MODELS,\n",
    "        \"analysis_metadata\": {\n",
    "            \"max_texts_per_dataset\": 2000,\n",
    "            \"reference_model\": None,\n",
    "            \"projection_dimension\": None,\n",
    "            \"formula_version\": \"theorem_formula_with_transformed_means\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for result in results:\n",
    "        dataset_name = result[\"dataset_name\"]\n",
    "        save_data[\"datasets\"][dataset_name] = {\n",
    "            \"model_data\": result[\"model_data\"],\n",
    "            \"reference_model\": result[\"reference_model\"],\n",
    "            \"reference_mean\": result[\"reference_mean\"],\n",
    "            \"reference_covariance\": result[\"reference_covariance\"],\n",
    "            \"summary_stats\": {\n",
    "                \"model_sizes\": result[\"model_sizes\"],\n",
    "                \"lower_bounds\": result[\"lower_bounds\"],\n",
    "                \"upper_bounds\": result[\"upper_bounds\"],\n",
    "                \"estimates\": result[\"estimates\"],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if save_data[\"analysis_metadata\"][\"reference_model\"] is None:\n",
    "            save_data[\"analysis_metadata\"][\"reference_model\"] = result[\n",
    "                \"reference_model\"\n",
    "            ]\n",
    "            save_data[\"analysis_metadata\"][\"projection_dimension\"] = result[\n",
    "                \"reference_mean\"\n",
    "            ].shape[0]\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(save_data, f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name, dataset_config in DATASETS.items():\n",
    "    result = analyze_dataset(dataset_name, dataset_config)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "\n",
    "if results:\n",
    "    save_results(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class MultiDatasetCKAAnalyzer:\n",
    "    def __init__(self, results_file: str = \"cache/bert_igw_results.pkl\"):\n",
    "        self.results_file = results_file\n",
    "        self.load_existing_results()\n",
    "        self.bert_models = self.existing_results[\"bert_models\"]\n",
    "\n",
    "    def load_existing_results(self):\n",
    "        with open(self.results_file, \"rb\") as f:\n",
    "            self.existing_results = pickle.load(f)\n",
    "\n",
    "    def prepare_sample_dataset(self, dataset_name: str, max_samples: int = 1000):\n",
    "        dataset_configs = {\n",
    "            \"amazon_polarity\": (\"amazon_polarity\", \"test[:1000]\"),\n",
    "            \"yelp_review\": (\"yelp_review_full\", \"test[:1000]\"),\n",
    "            \"imdb\": (\"imdb\", \"test[:1000]\"),\n",
    "            \"ag_news\": (\"ag_news\", \"test[:1000]\"),\n",
    "        }\n",
    "\n",
    "        if dataset_name not in dataset_configs:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "        dataset_path, split = dataset_configs[dataset_name]\n",
    "\n",
    "        if split.startswith(\"test[\"):\n",
    "            dataset = load_dataset(dataset_path, split=split)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_path, split=split)\n",
    "\n",
    "        if len(dataset) > max_samples:\n",
    "            dataset = dataset.shuffle(seed=42).select(range(max_samples))\n",
    "\n",
    "        if \"text\" in dataset.features:\n",
    "            texts = dataset[\"text\"]\n",
    "        elif \"content\" in dataset.features:\n",
    "            texts = dataset[\"content\"]\n",
    "        elif \"label\" in dataset.features and len(dataset.features) == 2:\n",
    "            text_column = [col for col in dataset.features if col != \"label\"][0]\n",
    "            texts = dataset[text_column]\n",
    "        else:\n",
    "            text_columns = [\n",
    "                col for col, feat in dataset.features.items() if feat.dtype == \"string\"\n",
    "            ]\n",
    "            if not text_columns:\n",
    "                raise ValueError(f\"No text column found in {dataset_name}\")\n",
    "            texts = dataset[text_columns[0]]\n",
    "\n",
    "        print(f\"Sample size: {len(texts)} texts\")\n",
    "        return texts\n",
    "\n",
    "    def center_gram_matrix(self, K):\n",
    "        n = K.shape[0]\n",
    "\n",
    "        ones = torch.ones(n, 1, device=K.device)\n",
    "        H = torch.eye(n, device=K.device) - (1 / n) * torch.mm(ones, ones.t())\n",
    "\n",
    "        K_centered = torch.mm(torch.mm(H, K), H)\n",
    "        return K_centered\n",
    "\n",
    "    def compute_cka(self, X, Y):\n",
    "        X = X.float()\n",
    "        Y = Y.float()\n",
    "\n",
    "        K_X = torch.mm(X, X.t())\n",
    "        K_Y = torch.mm(Y, Y.t())\n",
    "\n",
    "        K_X_centered = self.center_gram_matrix(K_X)\n",
    "        K_Y_centered = self.center_gram_matrix(K_Y)\n",
    "\n",
    "        numerator = torch.trace(torch.mm(K_X_centered, K_Y_centered))\n",
    "\n",
    "        norm_X = torch.trace(torch.mm(K_X_centered, K_X_centered))\n",
    "        norm_Y = torch.trace(torch.mm(K_Y_centered, K_Y_centered))\n",
    "        denominator = torch.sqrt(norm_X * norm_Y)\n",
    "\n",
    "        if denominator < 1e-12:\n",
    "            return 0.0\n",
    "\n",
    "        cka = numerator / denominator\n",
    "        return cka.item()\n",
    "\n",
    "    def extract_representations(\n",
    "        self, model, tokenizer, texts, layer_idx=-2, max_length=512\n",
    "    ):\n",
    "        model.eval()\n",
    "        representations = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(texts, desc=\"Extracting representations\", leave=False):\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                ).to(model.device)\n",
    "\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                hidden_states = (\n",
    "                    outputs.hidden_states\n",
    "                )\n",
    "\n",
    "                layer_output = hidden_states[\n",
    "                    layer_idx\n",
    "                ]\n",
    "                cls_representation = layer_output[0, 0, :]\n",
    "\n",
    "                representations.append(cls_representation.cpu())\n",
    "\n",
    "        return torch.stack(representations)\n",
    "\n",
    "    def load_model_safely(self, model_name, model_path):\n",
    "        try:\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            model.to(device)\n",
    "            return model, tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def compute_all_cka_scores(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        max_models: int = None,\n",
    "        reference_model: str = \"bert-12-768\",\n",
    "        layer_idx: int = -2,\n",
    "    ):\n",
    "        texts = self.prepare_sample_dataset(dataset_name)\n",
    "\n",
    "        dataset_results = self.existing_results[\"datasets\"][dataset_name]\n",
    "        model_data = dataset_results[\"model_data\"]\n",
    "\n",
    "        comparison_models = {\n",
    "            k: v for k, v in model_data.items() if k != reference_model\n",
    "        }\n",
    "\n",
    "        model_items = list(comparison_models.items())\n",
    "        model_items.sort(key=lambda x: x[1][\"param_count\"])\n",
    "\n",
    "        if max_models is not None:\n",
    "            model_items = model_items[:max_models]\n",
    "            print(\n",
    "                f\"Only analyzing {max_models} smallest models\"\n",
    "            )\n",
    "\n",
    "        reference_path = self.bert_models[reference_model][0]\n",
    "        ref_model, ref_tokenizer = self.load_model_safely(\n",
    "            reference_model, reference_path\n",
    "        )\n",
    "\n",
    "        if ref_model is None:\n",
    "            print(f\"Failed to load reference model {reference_model}\")\n",
    "            return None\n",
    "\n",
    "        ref_representations = self.extract_representations(\n",
    "            ref_model, ref_tokenizer, texts, layer_idx\n",
    "        )\n",
    "\n",
    "        del ref_model, ref_tokenizer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        results = {\n",
    "            \"model_name\": [],\n",
    "            \"param_count\": [],\n",
    "            \"igw_distance\": [],\n",
    "            \"cka_score\": [],\n",
    "            \"load_success\": [],\n",
    "            \"dataset\": [],\n",
    "        }\n",
    "\n",
    "        for i, (model_name, data) in enumerate(model_items):\n",
    "            print(f\"\\n--- Processing model {i+1}/{len(model_items)}: {model_name} ---\")\n",
    "\n",
    "            param_count = data[\"param_count\"]\n",
    "\n",
    "            igw_distance = (\n",
    "                data[\"estimate\"]\n",
    "                if not np.isnan(data[\"estimate\"])\n",
    "                else data[\"upper_bound\"]\n",
    "            )\n",
    "            model_path = self.bert_models[model_name][0]\n",
    "\n",
    "            print(f\"Parameters: {param_count:.1f}M\")\n",
    "            print(f\"IGW Distance: {igw_distance:.4f}\")\n",
    "\n",
    "            model, tokenizer = self.load_model_safely(model_name, model_path)\n",
    "\n",
    "            if model is None:\n",
    "                results[\"model_name\"].append(model_name)\n",
    "                results[\"param_count\"].append(param_count)\n",
    "                results[\"igw_distance\"].append(igw_distance)\n",
    "                results[\"cka_score\"].append(0.0)\n",
    "                results[\"load_success\"].append(False)\n",
    "                results[\"dataset\"].append(dataset_name)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                model_representations = self.extract_representations(\n",
    "                    model, tokenizer, texts, layer_idx\n",
    "                )\n",
    "\n",
    "                cka_score = self.compute_cka(ref_representations, model_representations)\n",
    "\n",
    "                print(f\"CKA Score: {cka_score:.4f}\")\n",
    "\n",
    "                results[\"model_name\"].append(model_name)\n",
    "                results[\"param_count\"].append(param_count)\n",
    "                results[\"igw_distance\"].append(igw_distance)\n",
    "                results[\"cka_score\"].append(cka_score)\n",
    "                results[\"load_success\"].append(True)\n",
    "                results[\"dataset\"].append(dataset_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing CKA for {model_name}: {e}\")\n",
    "                results[\"model_name\"].append(model_name)\n",
    "                results[\"param_count\"].append(param_count)\n",
    "                results[\"igw_distance\"].append(igw_distance)\n",
    "                results[\"cka_score\"].append(0.0)\n",
    "                results[\"load_success\"].append(False)\n",
    "                results[\"dataset\"].append(dataset_name)\n",
    "\n",
    "            finally:\n",
    "                del model, tokenizer\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def save_cka_results(self, results_df: pd.DataFrame, dataset_name: str):\n",
    "        csv_filename = f\"cache/cka_igw_results_{dataset_name}.csv\"\n",
    "        pickle_filename = f\"cache/cka_igw_results_{dataset_name}.pkl\"\n",
    "\n",
    "        results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        with open(pickle_filename, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"results_dataframe\": results_df,\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"analysis_type\": \"CKA_vs_IGW\",\n",
    "                    \"bert_models\": self.bert_models,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "\n",
    "\n",
    "analyzer = MultiDatasetCKAAnalyzer()\n",
    "\n",
    "max_models = None\n",
    "layer_idx = -1\n",
    "datasets_to_analyze = [\"amazon_polarity\", \"ag_news\"]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for i, dataset_name in enumerate(datasets_to_analyze):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\n",
    "        f\"Processing dataset {i+1}/{len(datasets_to_analyze)}: {dataset_name.upper()}\"\n",
    "    )\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if dataset_name not in analyzer.existing_results[\"datasets\"]:\n",
    "        print(f\"Warning: {dataset_name} not found in IGW results\")\n",
    "        continue\n",
    "\n",
    "    results_df = analyzer.compute_all_cka_scores(\n",
    "        dataset_name=dataset_name, max_models=max_models, layer_idx=layer_idx\n",
    "    )\n",
    "\n",
    "    if results_df is not None:\n",
    "        analyzer.save_cka_results(results_df, dataset_name)\n",
    "        all_results[dataset_name] = results_df\n",
    "\n",
    "        successful = len(results_df[results_df[\"load_success\"]])\n",
    "        failed = len(results_df) - successful\n",
    "\n",
    "        if successful > 0:\n",
    "            successful_data = results_df[results_df[\"load_success\"]]\n",
    "            best_cka = successful_data.loc[successful_data[\"cka_score\"].idxmax()]\n",
    "            worst_cka = successful_data.loc[successful_data[\"cka_score\"].idxmin()]\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to process dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6170a",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "plt.style.use(\"math.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_single_dataset_igw(result, figsize=(8, 6)):\n",
    "    sizes = np.array(\n",
    "        [\n",
    "            4.4,\n",
    "            9.7,\n",
    "            22.8,\n",
    "            39.2,\n",
    "            4.8,\n",
    "            11.3,\n",
    "            29.1,\n",
    "            53.4,\n",
    "            5.2,\n",
    "            12.8,\n",
    "            35.4,\n",
    "            67.5,\n",
    "            5.6,\n",
    "            14.4,\n",
    "            41.7,\n",
    "            81.7,\n",
    "            6.0,\n",
    "            16.0,\n",
    "            48.0,\n",
    "            95.9,\n",
    "            6.4,\n",
    "            17.6,\n",
    "            54.3,\n",
    "            110.1,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lower = np.array(result[\"lower_bounds\"])\n",
    "    upper = np.array(result[\"upper_bounds\"])\n",
    "    corrected = np.array(result[\"estimates\"])\n",
    "    dataset_name = result[\"dataset_name\"]\n",
    "\n",
    "    n_models = len(lower)\n",
    "    sizes = sizes[:n_models]\n",
    "\n",
    "    sorted_indices = np.argsort(sizes)\n",
    "    sizes = sizes[sorted_indices]\n",
    "    lower = lower[sorted_indices]\n",
    "    upper = upper[sorted_indices]\n",
    "    corrected = corrected[sorted_indices]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(sizes, upper, \"o--\", label=\"Analytic upper bound\")\n",
    "\n",
    "    valid_mask = ~np.isnan(corrected)\n",
    "    if np.any(valid_mask):\n",
    "        plt.plot(\n",
    "            sizes[valid_mask],\n",
    "            corrected[valid_mask],\n",
    "            \"o-\",\n",
    "            label=\"RGD upper bound\",\n",
    "            markersize=5,\n",
    "        )\n",
    "\n",
    "        plt.plot(sizes, lower, \"o-\", label=\"Analytic lower bound\")\n",
    "\n",
    "        plt.fill_between(\n",
    "            sizes[valid_mask],\n",
    "            lower[valid_mask],\n",
    "            corrected[valid_mask],\n",
    "            alpha=0.3,\n",
    "            label=\"IGW distance range\",\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Distillation size (millions of parameters)\", fontsize=12)\n",
    "    plt.ylabel(\"IGW distance from bert-base\", fontsize=12)\n",
    "    plt.title(f\"IGW distance between distillations and bert-base ({dataset_name})\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/igw_distance_{dataset_name}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('cache/bert_igw_results.pkl', 'rb') as f:\n",
    "    saved_data = pickle.load(f)\n",
    "\n",
    "dataset_name = 'ag_news'\n",
    "if dataset_name in saved_data['datasets']:\n",
    "    result = {\n",
    "        'lower_bounds': saved_data['datasets'][dataset_name]['summary_stats']['lower_bounds'],\n",
    "        'upper_bounds': saved_data['datasets'][dataset_name]['summary_stats']['upper_bounds'],\n",
    "        'estimates': saved_data['datasets'][dataset_name]['summary_stats']['estimates'],\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "    plot_single_dataset_igw(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d86aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "def plot_single_dataset_cka(result, figsize=(8, 6)):\n",
    "    sizes = np.array(\n",
    "        [\n",
    "            4.4,\n",
    "            9.7,\n",
    "            22.8,\n",
    "            39.2,\n",
    "            4.8,\n",
    "            11.3,\n",
    "            29.1,\n",
    "            53.4,\n",
    "            5.2,\n",
    "            12.8,\n",
    "            35.4,\n",
    "            67.5,\n",
    "            5.6,\n",
    "            14.4,\n",
    "            41.7,\n",
    "            81.7,\n",
    "            6.0,\n",
    "            16.0,\n",
    "            48.0,\n",
    "            95.9,\n",
    "            6.4,\n",
    "            17.6,\n",
    "            54.3,\n",
    "            110.1,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_counts = np.array(result[\"param_counts\"])\n",
    "    igw_distances = np.array(result[\"igw_distances\"])\n",
    "    cka_scores = np.array(result[\"cka_scores\"])\n",
    "    dataset_name = result[\"dataset_name\"]\n",
    "\n",
    "    n_models = len(param_counts)\n",
    "    sizes = sizes[:n_models]\n",
    "\n",
    "    sorted_indices = np.argsort(sizes)\n",
    "    sizes = sizes[sorted_indices]\n",
    "    igw_distances = igw_distances[sorted_indices]\n",
    "    cka_scores = cka_scores[sorted_indices]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        igw_distances,\n",
    "        cka_scores,\n",
    "        c=sizes,\n",
    "        s=60,\n",
    "        alpha=0.7,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    if len(igw_distances) > 2:\n",
    "        z = np.polyfit(igw_distances, cka_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(igw_distances.min(), igw_distances.max(), 100)\n",
    "        plt.plot(\n",
    "            x_trend,\n",
    "            p(x_trend),\n",
    "            \"r--\",\n",
    "            alpha=0.8,\n",
    "            linewidth=2,\n",
    "            label=\"Trendline\",\n",
    "        )\n",
    "\n",
    "        corr_pearson, p_pearson = pearsonr(igw_distances, cka_scores)\n",
    "        corr_spearman, p_spearman = spearmanr(igw_distances, cka_scores)\n",
    "\n",
    "        corr_text = f\"Pearson $r$: {corr_pearson:.3f} ($p={p_pearson:.3f}$)\\nSpearman $\\\\rho$: {corr_spearman:.3f} ($p={p_spearman:.3f}$)\"\n",
    "        plt.text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            corr_text,\n",
    "            transform=plt.gca().transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            fontsize=12,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "        )\n",
    "\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Model size (millions of parameters)\", fontsize=12)\n",
    "\n",
    "    plt.xlabel(\"IGW distance from bert-base (RGD)\", fontsize=12)\n",
    "    plt.ylabel(\"CKA similarity with bert-base\", fontsize=12)\n",
    "    plt.title(f\"IGW distance vs. CKA similarity ({dataset_name})\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.ylim(top=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/cka_vs_igw_{dataset_name}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "dataset_name = \"amazon_polarity\"\n",
    "pickle_filename = f\"cache/cka_igw_results_{dataset_name}.pkl\"\n",
    "\n",
    "with open(pickle_filename, \"rb\") as f:\n",
    "    cka_data = pickle.load(f)\n",
    "    results_df = cka_data[\"results_dataframe\"]\n",
    "\n",
    "successful_results = results_df[results_df[\"load_success\"]].copy()\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    result = {\n",
    "        \"param_counts\": successful_results[\"param_count\"].values,\n",
    "        \"igw_distances\": successful_results[\"igw_distance\"].values,\n",
    "        \"cka_scores\": successful_results[\"cka_score\"].values,\n",
    "        \"dataset_name\": dataset_name,\n",
    "    }\n",
    "    plot_single_dataset_cka(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
